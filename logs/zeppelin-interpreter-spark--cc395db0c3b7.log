 INFO [2019-01-25 22:16:54,020] ({Thread-0} RemoteInterpreterServer.java[run]:97) - Starting remote interpreter server on port 45385
 INFO [2019-01-25 22:16:55,238] ({pool-1-thread-2} RemoteInterpreterServer.java[createInterpreter]:198) - Instantiate interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2019-01-25 22:16:55,284] ({pool-1-thread-2} RemoteInterpreterServer.java[createInterpreter]:198) - Instantiate interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2019-01-25 22:16:55,296] ({pool-1-thread-2} RemoteInterpreterServer.java[createInterpreter]:198) - Instantiate interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2019-01-25 22:16:55,334] ({pool-1-thread-2} RemoteInterpreterServer.java[createInterpreter]:198) - Instantiate interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2019-01-25 22:16:55,342] ({pool-1-thread-2} RemoteInterpreterServer.java[createInterpreter]:198) - Instantiate interpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2019-01-25 22:16:55,622] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job remoteInterpretJob_1548454615620 started by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:17:06,957] ({pool-2-thread-4} SparkInterpreter.java[createSparkSession]:318) - ------ Create new SparkContext local[*] -------
 WARN [2019-01-25 22:17:06,983] ({pool-2-thread-4} SparkInterpreter.java[setupConfForSparkR]:577) - sparkr.zip is not found, sparkr may not work.
 INFO [2019-01-25 22:17:09,759] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Running Spark version 2.1.0
 WARN [2019-01-25 22:17:10,697] ({pool-2-thread-4} NativeCodeLoader.java[<clinit>]:62) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
 WARN [2019-01-25 22:17:10,876] ({pool-2-thread-4} Logging.scala[logWarning]:66) - 
SPARK_CLASSPATH was detected (set to ':/zeppelin/interpreter/spark/dep/*:/zeppelin/interpreter/spark/*:/zeppelin/lib/interpreter/*:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
 WARN [2019-01-25 22:17:10,880] ({pool-2-thread-4} Logging.scala[logWarning]:66) - Setting 'spark.executor.extraClassPath' to ':/zeppelin/interpreter/spark/dep/*:/zeppelin/interpreter/spark/*:/zeppelin/lib/interpreter/*:' as a work-around.
 WARN [2019-01-25 22:17:10,881] ({pool-2-thread-4} Logging.scala[logWarning]:66) - Setting 'spark.driver.extraClassPath' to ':/zeppelin/interpreter/spark/dep/*:/zeppelin/interpreter/spark/*:/zeppelin/lib/interpreter/*:' as a work-around.
 INFO [2019-01-25 22:17:11,019] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Changing view acls to: root
 INFO [2019-01-25 22:17:11,022] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Changing modify acls to: root
 INFO [2019-01-25 22:17:11,035] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Changing view acls groups to: 
 INFO [2019-01-25 22:17:11,038] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Changing modify acls groups to: 
 INFO [2019-01-25 22:17:11,039] ({pool-2-thread-4} Logging.scala[logInfo]:54) - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
 INFO [2019-01-25 22:17:12,044] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Successfully started service 'sparkDriver' on port 46291.
 INFO [2019-01-25 22:17:12,269] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Registering MapOutputTracker
 INFO [2019-01-25 22:17:12,342] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Registering BlockManagerMaster
 INFO [2019-01-25 22:17:12,355] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
 INFO [2019-01-25 22:17:12,358] ({pool-2-thread-4} Logging.scala[logInfo]:54) - BlockManagerMasterEndpoint up
 INFO [2019-01-25 22:17:12,390] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Created local directory at /tmp/blockmgr-05518886-1466-4e6c-b34a-9abce8d7d2c9
 INFO [2019-01-25 22:17:12,493] ({pool-2-thread-4} Logging.scala[logInfo]:54) - MemoryStore started with capacity 408.9 MB
 INFO [2019-01-25 22:17:12,650] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Registering OutputCommitCoordinator
 INFO [2019-01-25 22:17:12,940] ({pool-2-thread-4} Log.java[initialized]:186) - Logging initialized @19508ms
 INFO [2019-01-25 22:17:13,615] ({pool-2-thread-4} Server.java[doStart]:327) - jetty-9.2.z-SNAPSHOT
 INFO [2019-01-25 22:17:13,696] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@39b7243a{/jobs,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,698] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@334f282e{/jobs/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,700] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@651d7c1b{/jobs/job,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,703] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@60e631cd{/jobs/job/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,705] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@60f05f1f{/stages,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,708] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@2ab41577{/stages/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,709] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@2447599e{/stages/stage,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,711] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@757e7c82{/stages/stage/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,717] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@1714cd95{/stages/pool,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,719] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@1d016dad{/stages/pool/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,721] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@3145c3a{/storage,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,724] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@588aab6{/storage/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,726] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@34c1b7f7{/storage/rdd,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,728] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@223bd8c7{/storage/rdd/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,729] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@36fb536{/environment,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,731] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@43f272f7{/environment/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,736] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@7ab50b71{/executors,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,738] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@6341900{/executors/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,741] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@788cdaf5{/executors/threadDump,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,743] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@28397ad9{/executors/threadDump/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,760] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@7a05022c{/static,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,761] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@5c5fe875{/,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,764] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@421ada79{/api,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,768] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@21969f51{/jobs/job/kill,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,771] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@73d0fd2d{/stages/stage/kill,null,AVAILABLE}
 INFO [2019-01-25 22:17:13,844] ({pool-2-thread-4} AbstractConnector.java[doStart]:266) - Started ServerConnector@3be66207{HTTP/1.1}{0.0.0.0:4040}
 INFO [2019-01-25 22:17:13,868] ({pool-2-thread-4} Server.java[doStart]:379) - Started @20436ms
 INFO [2019-01-25 22:17:13,869] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Successfully started service 'SparkUI' on port 4040.
 INFO [2019-01-25 22:17:13,887] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Bound SparkUI to 0.0.0.0, and started at http://172.18.0.2:4040
 INFO [2019-01-25 22:17:14,408] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Added file file:/zeppelin/interpreter/spark/pyspark/pyspark.zip at file:/zeppelin/interpreter/spark/pyspark/pyspark.zip with timestamp 1548454634386
 INFO [2019-01-25 22:17:14,414] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Copying /zeppelin/interpreter/spark/pyspark/pyspark.zip to /tmp/spark-53629084-a428-45b3-99d7-d54e45d45568/userFiles-cc386b77-00da-42bc-aec4-cf30cdd9075f/pyspark.zip
 INFO [2019-01-25 22:17:14,583] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Added file file:/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip at file:/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip with timestamp 1548454634582
 INFO [2019-01-25 22:17:14,596] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Copying /zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip to /tmp/spark-53629084-a428-45b3-99d7-d54e45d45568/userFiles-cc386b77-00da-42bc-aec4-cf30cdd9075f/py4j-0.10.4-src.zip
 INFO [2019-01-25 22:17:14,773] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Created default pool default, schedulingMode: FIFO, minShare: 0, weight: 1
 INFO [2019-01-25 22:17:14,943] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Starting executor ID driver on host localhost
 INFO [2019-01-25 22:17:14,974] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Using REPL class URI: spark://172.18.0.2:46291/classes
 INFO [2019-01-25 22:17:15,051] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33937.
 INFO [2019-01-25 22:17:15,053] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Server created on 172.18.0.2:33937
 INFO [2019-01-25 22:17:15,072] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
 INFO [2019-01-25 22:17:15,078] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Registering BlockManager BlockManagerId(driver, 172.18.0.2, 33937, None)
 INFO [2019-01-25 22:17:15,085] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Registering block manager 172.18.0.2:33937 with 408.9 MB RAM, BlockManagerId(driver, 172.18.0.2, 33937, None)
 INFO [2019-01-25 22:17:15,102] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Registered BlockManager BlockManagerId(driver, 172.18.0.2, 33937, None)
 INFO [2019-01-25 22:17:15,106] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Initialized BlockManager: BlockManagerId(driver, 172.18.0.2, 33937, None)
 INFO [2019-01-25 22:17:15,569] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@507524dc{/metrics/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,617] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Warehouse path is 'file:/zeppelin/spark-warehouse'.
 INFO [2019-01-25 22:17:15,638] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@469dd629{/SQL,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,640] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@1bfed76d{/SQL/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,642] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@2880110d{/SQL/execution,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,643] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@112f83cb{/SQL/execution/json,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,646] ({pool-2-thread-4} ContextHandler.java[doStart]:744) - Started o.s.j.s.ServletContextHandler@77b7d0c7{/static/sql,null,AVAILABLE}
 INFO [2019-01-25 22:17:15,748] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
 INFO [2019-01-25 22:17:17,625] ({pool-2-thread-4} HiveMetaStore.java[newRawStore]:589) - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
 INFO [2019-01-25 22:17:17,680] ({pool-2-thread-4} ObjectStore.java[initialize]:289) - ObjectStore, initialize called
 INFO [2019-01-25 22:17:18,005] ({pool-2-thread-4} Log4JLogger.java[info]:77) - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
 INFO [2019-01-25 22:17:18,006] ({pool-2-thread-4} Log4JLogger.java[info]:77) - Property datanucleus.cache.level2 unknown - will be ignored
 INFO [2019-01-25 22:17:22,742] ({pool-2-thread-4} ObjectStore.java[getPMF]:370) - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
 INFO [2019-01-25 22:17:24,987] ({pool-2-thread-4} Log4JLogger.java[info]:77) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
 INFO [2019-01-25 22:17:24,993] ({pool-2-thread-4} Log4JLogger.java[info]:77) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
 INFO [2019-01-25 22:17:28,938] ({pool-2-thread-4} Log4JLogger.java[info]:77) - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
 INFO [2019-01-25 22:17:28,939] ({pool-2-thread-4} Log4JLogger.java[info]:77) - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
 INFO [2019-01-25 22:17:29,820] ({pool-2-thread-4} MetaStoreDirectSql.java[<init>]:139) - Using direct SQL, underlying DB is DERBY
 INFO [2019-01-25 22:17:29,829] ({pool-2-thread-4} ObjectStore.java[setConf]:272) - Initialized ObjectStore
 WARN [2019-01-25 22:17:30,256] ({pool-2-thread-4} ObjectStore.java[checkSchema]:6666) - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
 WARN [2019-01-25 22:17:30,657] ({pool-2-thread-4} ObjectStore.java[getDatabase]:568) - Failed to get database default, returning NoSuchObjectException
 INFO [2019-01-25 22:17:31,010] ({pool-2-thread-4} HiveMetaStore.java[createDefaultRoles_core]:663) - Added admin role in metastore
 INFO [2019-01-25 22:17:31,028] ({pool-2-thread-4} HiveMetaStore.java[createDefaultRoles_core]:672) - Added public role in metastore
 INFO [2019-01-25 22:17:31,522] ({pool-2-thread-4} HiveMetaStore.java[addAdminUsers_core]:712) - No user is added in admin role, since config is empty
 INFO [2019-01-25 22:17:31,919] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_all_databases
 INFO [2019-01-25 22:17:31,924] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
 INFO [2019-01-25 22:17:31,970] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_functions: db=default pat=*
 INFO [2019-01-25 22:17:31,975] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
 INFO [2019-01-25 22:17:31,978] ({pool-2-thread-4} Log4JLogger.java[info]:77) - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
 INFO [2019-01-25 22:17:32,839] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created HDFS directory: /tmp/hive/root
 INFO [2019-01-25 22:17:32,858] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created local directory: /tmp/root
 INFO [2019-01-25 22:17:32,886] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created local directory: /tmp/284b8204-80fd-40b6-a5d1-561f09f79d2f_resources
 INFO [2019-01-25 22:17:32,905] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created HDFS directory: /tmp/hive/root/284b8204-80fd-40b6-a5d1-561f09f79d2f
 INFO [2019-01-25 22:17:32,924] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created local directory: /tmp/root/284b8204-80fd-40b6-a5d1-561f09f79d2f
 INFO [2019-01-25 22:17:32,937] ({pool-2-thread-4} SessionState.java[createPath]:641) - Created HDFS directory: /tmp/hive/root/284b8204-80fd-40b6-a5d1-561f09f79d2f/_tmp_space.db
 INFO [2019-01-25 22:17:32,942] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Warehouse location for Hive client (version 1.2.1) is file:/zeppelin/spark-warehouse
 INFO [2019-01-25 22:17:32,967] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_database: default
 INFO [2019-01-25 22:17:33,008] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
 INFO [2019-01-25 22:17:33,064] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_database: global_temp
 INFO [2019-01-25 22:17:33,065] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
 WARN [2019-01-25 22:17:33,067] ({pool-2-thread-4} ObjectStore.java[getDatabase]:568) - Failed to get database global_temp, returning NoSuchObjectException
 INFO [2019-01-25 22:17:33,078] ({pool-2-thread-4} SparkInterpreter.java[createSparkSession]:369) - Created Spark session with Hive support
 INFO [2019-01-25 22:17:49,955] ({pool-2-thread-4} SparkInterpreter.java[populateSparkWebUrl]:1013) - Sending metainfos to Zeppelin server: {url=http://172.18.0.2:4040}
 INFO [2019-01-25 22:17:50,827] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job remoteInterpretJob_1548454615620 finished by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:18:20,854] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job remoteInterpretJob_1548454700850 started by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:18:21,165] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Parsing command: select * from game_sales
where Platform = 'NES' -- NESはファミコンのことです。
order by JP_Sales desc
limit 30
 INFO [2019-01-25 22:18:23,512] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_table : db=default tbl=game_sales
 INFO [2019-01-25 22:18:23,513] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=game_sales	
 INFO [2019-01-25 22:18:23,702] ({pool-2-thread-4} HiveMetaStore.java[logInfo]:746) - 0: get_table : db=default tbl=game_sales
 INFO [2019-01-25 22:18:23,703] ({pool-2-thread-4} HiveMetaStore.java[logAuditEvent]:371) - ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=game_sales	
ERROR [2019-01-25 22:18:23,707] ({pool-2-thread-4} SparkSqlInterpreter.java[interpret]:121) - Invocation target exception
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:116)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:498)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:175)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: game_sales; line 1 pos 14
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:459)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:478)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:463)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:58)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:463)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:453)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:592)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:699)
	... 16 more
 INFO [2019-01-25 22:18:23,728] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job remoteInterpretJob_1548454700850 finished by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:19:21,911] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:131) - Job remoteInterpretJob_1548454761910 started by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:19:27,024] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_0 stored as values in memory (estimated size 208.5 KB, free 408.7 MB)
 INFO [2019-01-25 22:19:27,379] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.0 KB, free 408.7 MB)
 INFO [2019-01-25 22:19:27,382] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_0_piece0 in memory on 172.18.0.2:33937 (size: 20.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:27,406] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 0 from csv at <console>:33
 INFO [2019-01-25 22:19:27,781] ({pool-2-thread-5} FileInputFormat.java[listStatus]:247) - Total input paths to process : 1
 INFO [2019-01-25 22:19:27,953] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: csv at <console>:33
 INFO [2019-01-25 22:19:27,983] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 0 (csv at <console>:33) with 1 output partitions
 INFO [2019-01-25 22:19:27,985] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 0 (csv at <console>:33)
 INFO [2019-01-25 22:19:27,986] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2019-01-25 22:19:27,992] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2019-01-25 22:19:28,012] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at <console>:33), which has no missing parents
 INFO [2019-01-25 22:19:28,068] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1 stored as values in memory (estimated size 3.5 KB, free 408.7 MB)
 INFO [2019-01-25 22:19:28,093] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.0 KB, free 408.7 MB)
 INFO [2019-01-25 22:19:28,095] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_1_piece0 in memory on 172.18.0.2:33937 (size: 2.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:28,111] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 1 from broadcast at DAGScheduler.scala:996
 INFO [2019-01-25 22:19:28,124] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at <console>:33)
 INFO [2019-01-25 22:19:28,132] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 0.0 with 1 tasks
 INFO [2019-01-25 22:19:28,161] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_0.0 tasks to pool default
 INFO [2019-01-25 22:19:28,427] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes)
 INFO [2019-01-25 22:19:28,465] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Running task 0.0 in stage 0.0 (TID 0)
 INFO [2019-01-25 22:19:28,483] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Fetching file:/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip with timestamp 1548454634582
 INFO [2019-01-25 22:19:28,696] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - /zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip has been previously copied to /tmp/spark-53629084-a428-45b3-99d7-d54e45d45568/userFiles-cc386b77-00da-42bc-aec4-cf30cdd9075f/py4j-0.10.4-src.zip
 INFO [2019-01-25 22:19:28,787] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Fetching file:/zeppelin/interpreter/spark/pyspark/pyspark.zip with timestamp 1548454634386
 INFO [2019-01-25 22:19:28,813] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - /zeppelin/interpreter/spark/pyspark/pyspark.zip has been previously copied to /tmp/spark-53629084-a428-45b3-99d7-d54e45d45568/userFiles-cc386b77-00da-42bc-aec4-cf30cdd9075f/pyspark.zip
 INFO [2019-01-25 22:19:28,994] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Input split: file:/data/video.csv:0+757978
 INFO [2019-01-25 22:19:29,238] ({Executor task launch worker-0} Configuration.java[warnOnceIfDeprecated]:1049) - mapred.tip.id is deprecated. Instead, use mapreduce.task.id
 INFO [2019-01-25 22:19:29,245] ({Executor task launch worker-0} Configuration.java[warnOnceIfDeprecated]:1049) - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
 INFO [2019-01-25 22:19:29,246] ({Executor task launch worker-0} Configuration.java[warnOnceIfDeprecated]:1049) - mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
 INFO [2019-01-25 22:19:29,247] ({Executor task launch worker-0} Configuration.java[warnOnceIfDeprecated]:1049) - mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
 INFO [2019-01-25 22:19:29,248] ({Executor task launch worker-0} Configuration.java[warnOnceIfDeprecated]:1049) - mapred.job.id is deprecated. Instead, use mapreduce.job.id
 INFO [2019-01-25 22:19:29,317] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 0.0 (TID 0). 1240 bytes result sent to driver
 INFO [2019-01-25 22:19:29,365] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 0 (csv at <console>:33) finished in 1.162 s
 INFO [2019-01-25 22:19:29,370] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 0.0 (TID 0) in 1080 ms on localhost (executor driver) (1/1)
 INFO [2019-01-25 22:19:29,387] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 0.0, whose tasks have all completed, from pool default
 INFO [2019-01-25 22:19:29,413] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 0 finished: csv at <console>:33, took 1.459074 s
 INFO [2019-01-25 22:19:29,578] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_2 stored as values in memory (estimated size 208.5 KB, free 408.5 MB)
 INFO [2019-01-25 22:19:29,600] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.0 KB, free 408.4 MB)
 INFO [2019-01-25 22:19:29,602] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_2_piece0 in memory on 172.18.0.2:33937 (size: 20.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:29,609] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 2 from csv at <console>:33
 INFO [2019-01-25 22:19:29,644] ({pool-2-thread-5} FileInputFormat.java[listStatus]:247) - Total input paths to process : 1
 INFO [2019-01-25 22:19:29,654] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: csv at <console>:33
 INFO [2019-01-25 22:19:29,656] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 1 (csv at <console>:33) with 1 output partitions
 INFO [2019-01-25 22:19:29,658] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 1 (csv at <console>:33)
 INFO [2019-01-25 22:19:29,659] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2019-01-25 22:19:29,660] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2019-01-25 22:19:29,662] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 1 (MapPartitionsRDD[5] at csv at <console>:33), which has no missing parents
 INFO [2019-01-25 22:19:29,666] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3 stored as values in memory (estimated size 3.5 KB, free 408.4 MB)
 INFO [2019-01-25 22:19:29,668] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.0 KB, free 408.4 MB)
 INFO [2019-01-25 22:19:29,671] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_3_piece0 in memory on 172.18.0.2:33937 (size: 2.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:29,677] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 3 from broadcast at DAGScheduler.scala:996
 INFO [2019-01-25 22:19:29,678] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at csv at <console>:33)
 INFO [2019-01-25 22:19:29,682] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 1.0 with 1 tasks
 INFO [2019-01-25 22:19:29,684] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_1.0 tasks to pool default
 INFO [2019-01-25 22:19:29,688] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 6229 bytes)
 INFO [2019-01-25 22:19:29,689] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Running task 0.0 in stage 1.0 (TID 1)
 INFO [2019-01-25 22:19:29,705] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Input split: file:/data/video.csv:0+757978
 INFO [2019-01-25 22:19:29,752] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 1.0 (TID 1). 1240 bytes result sent to driver
 INFO [2019-01-25 22:19:29,755] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 1 (csv at <console>:33) finished in 0.070 s
 INFO [2019-01-25 22:19:29,759] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 1 finished: csv at <console>:33, took 0.103706 s
 INFO [2019-01-25 22:19:29,757] ({task-result-getter-1} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on localhost (executor driver) (1/1)
 INFO [2019-01-25 22:19:29,766] ({task-result-getter-1} Logging.scala[logInfo]:54) - Removed TaskSet 1.0, whose tasks have all completed, from pool default
 INFO [2019-01-25 22:19:29,847] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Starting job: csv at <console>:33
 INFO [2019-01-25 22:19:29,852] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 2 (csv at <console>:33) with 2 output partitions
 INFO [2019-01-25 22:19:29,853] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 2 (csv at <console>:33)
 INFO [2019-01-25 22:19:29,867] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2019-01-25 22:19:29,868] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2019-01-25 22:19:29,870] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 2 (MapPartitionsRDD[6] at csv at <console>:33), which has no missing parents
 INFO [2019-01-25 22:19:29,888] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4 stored as values in memory (estimated size 6.0 KB, free 408.4 MB)
 INFO [2019-01-25 22:19:29,919] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KB, free 408.4 MB)
 INFO [2019-01-25 22:19:29,932] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_4_piece0 in memory on 172.18.0.2:33937 (size: 3.6 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:29,951] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 4 from broadcast at DAGScheduler.scala:996
 INFO [2019-01-25 22:19:29,953] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at csv at <console>:33)
 INFO [2019-01-25 22:19:29,953] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 2.0 with 2 tasks
 INFO [2019-01-25 22:19:29,955] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_2.0 tasks to pool default
 INFO [2019-01-25 22:19:29,978] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 6234 bytes)
 INFO [2019-01-25 22:19:29,981] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 1.0 in stage 2.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 6234 bytes)
 INFO [2019-01-25 22:19:29,983] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Running task 0.0 in stage 2.0 (TID 2)
 INFO [2019-01-25 22:19:29,995] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Input split: file:/data/video.csv:0+757978
 INFO [2019-01-25 22:19:30,003] ({Executor task launch worker-1} Logging.scala[logInfo]:54) - Running task 1.0 in stage 2.0 (TID 3)
 INFO [2019-01-25 22:19:30,028] ({Executor task launch worker-1} Logging.scala[logInfo]:54) - Input split: file:/data/video.csv:757978+757979
 INFO [2019-01-25 22:19:30,940] ({Executor task launch worker-1} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 2.0 (TID 3). 1420 bytes result sent to driver
 INFO [2019-01-25 22:19:30,956] ({task-result-getter-2} Logging.scala[logInfo]:54) - Finished task 1.0 in stage 2.0 (TID 3) in 975 ms on localhost (executor driver) (1/2)
 INFO [2019-01-25 22:19:30,958] ({Executor task launch worker-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 2.0 (TID 2). 1333 bytes result sent to driver
 INFO [2019-01-25 22:19:30,966] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 2 (csv at <console>:33) finished in 1.010 s
 INFO [2019-01-25 22:19:30,969] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Job 2 finished: csv at <console>:33, took 1.117580 s
 INFO [2019-01-25 22:19:30,967] ({task-result-getter-3} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 2.0 (TID 2) in 1006 ms on localhost (executor driver) (2/2)
 INFO [2019-01-25 22:19:30,981] ({task-result-getter-3} Logging.scala[logInfo]:54) - Removed TaskSet 2.0, whose tasks have all completed, from pool default
 INFO [2019-01-25 22:19:33,308] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Pruning directories with: 
 INFO [2019-01-25 22:19:33,320] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Post-Scan Filters: 
 INFO [2019-01-25 22:19:33,335] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Output Data Schema: struct<Name: string, Platform: string, Year_of_Release: string, Genre: string, Publisher: string ... 13 more fields>
 INFO [2019-01-25 22:19:33,340] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Pushed Filters: 
 INFO [2019-01-25 22:19:34,201] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Code generated in 552.183935 ms
 INFO [2019-01-25 22:19:34,248] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_5 stored as values in memory (estimated size 226.2 KB, free 408.2 MB)
 INFO [2019-01-25 22:19:34,319] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.9 KB, free 408.2 MB)
 INFO [2019-01-25 22:19:34,321] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Added broadcast_5_piece0 in memory on 172.18.0.2:33937 (size: 20.9 KB, free: 408.8 MB)
 INFO [2019-01-25 22:19:34,332] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Created broadcast 5 from cache at <console>:31
 INFO [2019-01-25 22:19:34,364] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
 INFO [2019-01-25 22:19:34,784] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_0_piece0 on 172.18.0.2:33937 in memory (size: 20.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:34,805] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_1_piece0 on 172.18.0.2:33937 in memory (size: 2.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:34,808] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_2_piece0 on 172.18.0.2:33937 in memory (size: 20.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:34,811] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_3_piece0 on 172.18.0.2:33937 in memory (size: 2.0 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:34,819] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Removed broadcast_4_piece0 on 172.18.0.2:33937 in memory (size: 3.6 KB, free: 408.9 MB)
 INFO [2019-01-25 22:19:35,682] ({pool-2-thread-5} Logging.scala[logInfo]:54) - Parsing command: game_sales
 INFO [2019-01-25 22:19:35,766] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:137) - Job remoteInterpretJob_1548454761910 finished by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:21:51,931] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job remoteInterpretJob_1548454911930 started by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
 INFO [2019-01-25 22:21:51,934] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Parsing command: select * from game_sales
where Platform = 'NES' -- NESはファミコンのことです。
order by JP_Sales desc
limit 30
 INFO [2019-01-25 22:21:52,332] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Predicate isnotnull(Platform#1) generates partition filter: ((Platform.count#163 - Platform.nullCount#162) > 0)
 INFO [2019-01-25 22:21:52,339] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Predicate (Platform#1 = NES) generates partition filter: ((Platform.lowerBound#161 <= NES) && (NES <= Platform.upperBound#160))
 INFO [2019-01-25 22:21:52,516] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Code generated in 40.830215 ms
 INFO [2019-01-25 22:21:52,715] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Code generated in 110.213406 ms
 INFO [2019-01-25 22:21:52,808] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Starting job: take at NativeMethodAccessorImpl.java:0
 INFO [2019-01-25 22:21:52,819] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Got job 3 (take at NativeMethodAccessorImpl.java:0) with 1 output partitions
 INFO [2019-01-25 22:21:52,820] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Final stage: ResultStage 3 (take at NativeMethodAccessorImpl.java:0)
 INFO [2019-01-25 22:21:52,821] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Parents of final stage: List()
 INFO [2019-01-25 22:21:52,826] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Missing parents: List()
 INFO [2019-01-25 22:21:52,828] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting ResultStage 3 (MapPartitionsRDD[14] at take at NativeMethodAccessorImpl.java:0), which has no missing parents
 INFO [2019-01-25 22:21:53,016] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6 stored as values in memory (estimated size 28.7 KB, free 408.6 MB)
 INFO [2019-01-25 22:21:53,035] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Block broadcast_6_piece0 stored as bytes in memory (estimated size 11.1 KB, free 408.6 MB)
 INFO [2019-01-25 22:21:53,040] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added broadcast_6_piece0 in memory on 172.18.0.2:33937 (size: 11.1 KB, free: 408.9 MB)
 INFO [2019-01-25 22:21:53,046] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Created broadcast 6 from broadcast at DAGScheduler.scala:996
 INFO [2019-01-25 22:21:53,048] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[14] at take at NativeMethodAccessorImpl.java:0)
 INFO [2019-01-25 22:21:53,049] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Adding task set 3.0 with 1 tasks
 INFO [2019-01-25 22:21:53,051] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - Added task set TaskSet_3.0 tasks to pool default
 INFO [2019-01-25 22:21:53,057] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 6771 bytes)
 INFO [2019-01-25 22:21:53,060] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Running task 0.0 in stage 3.0 (TID 4)
 INFO [2019-01-25 22:21:53,274] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Reading File path: file:///data/video.csv, range: 0-1515957, partition values: [empty row]
 INFO [2019-01-25 22:21:53,624] ({Executor task launch worker-2} TransportClientFactory.java[createClient]:250) - Successfully created connection to /172.18.0.2:46291 after 163 ms (0 ms spent in bootstraps)
 INFO [2019-01-25 22:21:53,941] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Code generated in 602.466754 ms
 INFO [2019-01-25 22:21:56,476] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Block rdd_9_0 stored as values in memory (estimated size 1541.8 KB, free 407.1 MB)
 INFO [2019-01-25 22:21:56,479] ({dispatcher-event-loop-1} Logging.scala[logInfo]:54) - Added rdd_9_0 in memory on 172.18.0.2:33937 (size: 1541.8 KB, free: 407.4 MB)
 INFO [2019-01-25 22:21:56,653] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Code generated in 61.634835 ms
 INFO [2019-01-25 22:21:56,855] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Code generated in 178.29253 ms
 INFO [2019-01-25 22:21:57,180] ({Executor task launch worker-2} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 3.0 (TID 4). 10042 bytes result sent to driver
 INFO [2019-01-25 22:21:57,243] ({dag-scheduler-event-loop} Logging.scala[logInfo]:54) - ResultStage 3 (take at NativeMethodAccessorImpl.java:0) finished in 4.191 s
 INFO [2019-01-25 22:21:57,244] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Job 3 finished: take at NativeMethodAccessorImpl.java:0, took 4.446730 s
 INFO [2019-01-25 22:21:57,256] ({task-result-getter-0} Logging.scala[logInfo]:54) - Finished task 0.0 in stage 3.0 (TID 4) in 4190 ms on localhost (executor driver) (1/1)
 INFO [2019-01-25 22:21:57,273] ({task-result-getter-0} Logging.scala[logInfo]:54) - Removed TaskSet 3.0, whose tasks have all completed, from pool default
 INFO [2019-01-25 22:21:57,391] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Code generated in 105.851033 ms
 INFO [2019-01-25 22:21:57,500] ({dispatcher-event-loop-0} Logging.scala[logInfo]:54) - Removed broadcast_6_piece0 on 172.18.0.2:33937 in memory (size: 11.1 KB, free: 407.4 MB)
 INFO [2019-01-25 22:21:57,565] ({pool-2-thread-4} Logging.scala[logInfo]:54) - Code generated in 44.266844 ms
 INFO [2019-01-25 22:21:57,589] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job remoteInterpretJob_1548454911930 finished by scheduler org.apache.zeppelin.spark.SparkInterpreter951205302
